{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.147 ðŸš€ Python-3.11.3 torch-2.0.1 CPU (Apple M1 Pro)\n",
      "Setup complete âœ… (10 CPUs, 16.0 GB RAM, 291.6/460.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!yolo task=detect mode=train model=yolov8n.pt data=../all_datasets/data_V/data.yaml epochs=30 imgsz=640 save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "imageFromCDataset = \"../localTesting/video3_9.jpg\"\n",
    "imageFromVDataset = \"../localTesting/fromModelVValid.jpg\"\n",
    "independentCameraView = \"../localTesting/independent_camera.jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelV = YOLO(\"models/V/weights/V.pt\")\n",
    "modelC = YOLO(\"models/C/weights/C.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/dogukan/repos/safety_ai/object_detector_model_training/../localTesting/video3_9.jpg: 640x224 1 vest, 35.0ms\n",
      "Speed: 1.1ms preprocess, 35.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict9\u001b[0m\n",
      "\n",
      "image 1/1 /Users/dogukan/repos/safety_ai/object_detector_model_training/../localTesting/video3_9.jpg: 640x224 1 helmet, 1 vest, 35.5ms\n",
      "Speed: 1.0ms preprocess, 35.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for an image from modelC's dataset (camera view),\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    resultsV = modelV(imageFromCDataset, save=True) # around 0.79 (trained with 'vest' images)\n",
    "    resultsC = modelC(imageFromCDataset, save=True) # around 0.86 (trained with camera view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/dogukan/repos/safety_ai/object_detector_model_training/../localTesting/fromModelVValid.jpg: 512x640 3 vests, 78.6ms\n",
      "Speed: 2.3ms preprocess, 78.6ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict7\u001b[0m\n",
      "\n",
      "image 1/1 /Users/dogukan/repos/safety_ai/object_detector_model_training/../localTesting/fromModelVValid.jpg: 512x640 2 vests, 59.2ms\n",
      "Speed: 2.0ms preprocess, 59.2ms inference, 0.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7937, 0.7465, 0.3282])\n"
     ]
    }
   ],
   "source": [
    "# for an image from modelV's dataset (camera view),\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    resultsV = modelV(imageFromVDataset, save=True) # detected 3 vests\n",
    "    resultsC = modelC(imageFromVDataset, save=True) # vests (3) are not detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7937, 0.7465, 0.3282])\n"
     ]
    }
   ],
   "source": [
    "print(resultsV[0].boxes.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/dogukan/repos/safety_ai/object_detector_model_training/../localTesting/independent_camera.jpeg: 352x640 8 vests, 65.5ms\n",
      "Speed: 3.9ms preprocess, 65.5ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict9\u001b[0m\n",
      "\n",
      "image 1/1 /Users/dogukan/repos/safety_ai/object_detector_model_training/../localTesting/independent_camera.jpeg: 352x640 (no detections), 49.2ms\n",
      "Speed: 1.8ms preprocess, 49.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1m/opt/homebrew/runs/detect/predict10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a prediction on an external image\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    resultsV = modelV(independentCameraView, save=True)\n",
    "    resultsC = modelC(independentCameraView, save=True) \n",
    "\n",
    "# model V detected 8/10 vests for a new camera view; with confidences between 0.4-0.79 (hiqh quality, not cropped image)\n",
    "# model C not detected any..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these results implies that a model with a combined dataset (C and V) would work more efficient in general. \n",
    "# ModelC by it's own, is not enough. \n",
    "# ModelV works fine in general, but since this is the main usage logic of our program, it should be tested on cropped camera images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
