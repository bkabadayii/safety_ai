{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 14:32:49.643640: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load face cascade\n",
    "face_cascade = cv2.CascadeClassifier('../xml_files/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the model\n",
    "model = load_model('../final_model/keras_model.h5')\n",
    "\n",
    "# CAMERA can be 0 or 1 based on default camera of your computer.\n",
    "# Next line may be used in order to get input from the camera.\n",
    "camera = cv2.VideoCapture(0)\n",
    "# camera.release()\n",
    "\n",
    "# Grab the labels from the labels.txt file. This will be used later.\n",
    "labels = open('../final_model/labels.txt', 'r').readlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP THE FACE:\n",
    "image_name = \"data13.jpg\"\n",
    "input_path = f'../data/{image_name}'\n",
    "output_path = f'../test_outputs/'\n",
    "\n",
    "# Load the image and convert it to grayscale\n",
    "image = cv2.imread(input_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "if (len(faces) == 0):\n",
    "    print(\"Face Not Found!\")\n",
    "\n",
    "cropped_faces = []\n",
    "# Iterate over the detected faces\n",
    "face_index = 0\n",
    "for (x,y,w,h) in faces:\n",
    "    # Crop the image (0.5 * length_face) cm above the face\n",
    "    y_offset = int(0.5 * h)  # 0.5 * length_face above the face\n",
    "    crop_img = image[y-y_offset:y+h, x:x+w]\n",
    "\n",
    "    # Save the cropped image to a file\n",
    "    try:\n",
    "        cv2.imwrite(f\"{output_path}cropped_{face_index}_{image_name}\", crop_img)\n",
    "        cropped_faces.append(crop_img)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    face_index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Sureness level: 65.43623208999634 %\n",
      "1 No Helmet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DETECT WHETHER THE FACE HAS HELMET:\n",
    "\n",
    "for image in cropped_faces:\n",
    "    # Resize the raw image into (224-height,224-width) pixls.\n",
    "    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # DEBUG\n",
    "    cv2.imwrite(f\"../test_preprocess/image/resized_{image_name}\", image)\n",
    "    # DEBUG\n",
    "      \n",
    "    # Make the image a numpy array and reshape it to the models input shape.\n",
    "    image = np.asarray(image, dtype=np.float32).reshape(1, 224, 224, 3)\n",
    "    \n",
    "    # Normalize the image array\n",
    "    image = (image / 127.5) - 1\n",
    "    \n",
    "    # Have the model predict what the current image is. Model.predict\n",
    "    # returns an array of percentages. Example:[0.2,0.8] meaning its 20% sure\n",
    "    # it is the first label and 80% sure its the second label.\n",
    "    probabilities = model.predict(image)\n",
    "    # Print what the highest value probabilitie label\n",
    "    print(f\"Sureness level: {max(probabilities[0]) * 100} %\")\n",
    "    print(labels[np.argmax(probabilities)])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Not Found!\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Sureness level: 93.78589391708374 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Sureness level: 95.14492154121399 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Sureness level: 95.23208737373352 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 96.35847210884094 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 97.99647331237793 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sureness level: 97.3625898361206 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sureness level: 95.84303498268127 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Sureness level: 92.7346408367157 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Sureness level: 93.68639588356018 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Sureness level: 95.4449713230133 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Sureness level: 96.63656949996948 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 94.79268789291382 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Sureness level: 96.76528573036194 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Sureness level: 94.95485424995422 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Sureness level: 96.76098227500916 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Sureness level: 95.88831663131714 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Sureness level: 94.62503790855408 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 95.92745900154114 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Sureness level: 94.9683666229248 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Sureness level: 88.09962272644043 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Sureness level: 96.03609442710876 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 95.5271303653717 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 96.16251587867737 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Sureness level: 98.05092811584473 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sureness level: 93.70999932289124 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 94.92461681365967 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Sureness level: 84.83119010925293 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 90.42627811431885 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Sureness level: 97.92565107345581 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Sureness level: 97.08037972450256 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Sureness level: 87.98079490661621 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sureness level: 86.78457140922546 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Sureness level: 95.63270211219788 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sureness level: 87.45163083076477 %\n",
      "1 No Helmet\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Sureness level: 91.29602313041687 %\n",
      "1 No Helmet\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m image \u001b[39m=\u001b[39m (image \u001b[39m/\u001b[39m \u001b[39m127.5\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Have the model predict what the current image is. Model.predict\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# returns an array of percentages. Example:[0.2,0.8] meaning its 20% sure\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# it is the first label and 80% sure its the second label.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m probabilities \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Print what the highest value probabilitie label\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/baturalpkabadayi/SU/202/proj201/HelmetDetection/source/detect_helmet.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSureness level: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mmax\u001b[39m(probabilities[\u001b[39m0\u001b[39m])\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m %\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/training.py:2317\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2309\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2310\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2311\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2314\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2315\u001b[0m         )\n\u001b[0;32m-> 2317\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2318\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2319\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2320\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2321\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2322\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2323\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2324\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2325\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2326\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2327\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2328\u001b[0m )\n\u001b[1;32m   2330\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:1579\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1578\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1579\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:1259\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1258\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1260\u001b[0m     x,\n\u001b[1;32m   1261\u001b[0m     y,\n\u001b[1;32m   1262\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1263\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1264\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1265\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1266\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1267\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1268\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1269\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1270\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1271\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1272\u001b[0m )\n\u001b[1;32m   1274\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/engine/data_adapter.py:290\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m indices_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mrange(\u001b[39m1\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 290\u001b[0m     indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mrepeat(epochs)\n\u001b[1;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpermutation\u001b[39m(_):\n\u001b[1;32m    293\u001b[0m     \u001b[39m# It turns out to be more performant to make a new set of indices\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[39m# rather than reusing the same range Tensor. (presumably because of\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[39m# buffer forwarding.)\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrange(num_samples, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:1513\u001b[0m, in \u001b[0;36mDatasetV2.repeat\u001b[0;34m(self, count, name)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat\u001b[39m(\u001b[39mself\u001b[39m, count\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1493\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Repeats this dataset so each original value is seen `count` times.\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \n\u001b[1;32m   1495\u001b[0m \u001b[39m  >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[39m    A new `Dataset` with the transformation applied as described above.\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1513\u001b[0m   \u001b[39mreturn\u001b[39;00m RepeatDataset(\u001b[39mself\u001b[39;49m, count, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5019\u001b[0m, in \u001b[0;36mRepeatDataset.__init__\u001b[0;34m(self, input_dataset, count, name)\u001b[0m\n\u001b[1;32m   5017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count \u001b[39m=\u001b[39m constant_op\u001b[39m.\u001b[39mconstant(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint64, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5018\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 5019\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[1;32m   5020\u001b[0m       count, dtype\u001b[39m=\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mint64, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   5021\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m   5022\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mrepeat_dataset(\n\u001b[1;32m   5023\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   5024\u001b[0m     count\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count,\n\u001b[1;32m   5025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:178\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner_wrapper\u001b[39m(func):\n\u001b[0;32m--> 178\u001b[0m   \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    179\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m enabled:\n\u001b[1;32m    181\u001b[0m       \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CROP THE FACE:\n",
    "\n",
    "# Number of iterations:\n",
    "num_iterations = 100\n",
    "\n",
    "# cam\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Load the image and convert it to grayscale\n",
    "    ret, image = camera.read()\n",
    "    \n",
    "    if ret == False:\n",
    "        continue\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('webcam', image)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    if (len(faces) == 0):\n",
    "        print(\"Face Not Found!\")\n",
    "\n",
    "    cropped_faces = []\n",
    "    # Iterate over the detected faces\n",
    "    face_index = 0\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Crop the image (0.5 * length_face) cm above the face\n",
    "        y_offset = int(0.5 * h)  # 0.5 * length_face above the face\n",
    "        crop_img = image[y-y_offset:y+h, x:x+w]\n",
    "\n",
    "        # Save the cropped image to a file\n",
    "        try:\n",
    "            cropped_faces.append(crop_img)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        face_index +=1\n",
    "\n",
    "    # DETECT WHETHER THE FACE HAS HELMET:\n",
    "\n",
    "    for image in cropped_faces:\n",
    "        # Resize the raw image into (224-height,224-width) pixels.\n",
    "        image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # DEBUG\n",
    "        cv2.imwrite(\"../test_preprocess/camera/resized_camera.jpg\", image)\n",
    "        # DEBUG\n",
    "      \n",
    "\n",
    "        # Make the image a numpy array and reshape it to the models input shape.\n",
    "        image = np.asarray(image, dtype=np.float32).reshape(1, 224, 224, 3)\n",
    "        # Normalize the image array\n",
    "        image = (image / 127.5) - 1\n",
    "        # Have the model predict what the current image is. Model.predict\n",
    "        # returns an array of percentages. Example:[0.2,0.8] meaning its 20% sure\n",
    "        # it is the first label and 80% sure its the second label.\n",
    "        probabilities = model.predict(image)\n",
    "        # Print what the highest value probabilitie label\n",
    "        print(f\"Sureness level: {max(probabilities[0]) * 100} %\")\n",
    "        print(labels[np.argmax(probabilities)])\n",
    "    \n",
    "    if (cv2.waitKey(1) == 27):\n",
    "       break\n",
    "    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
